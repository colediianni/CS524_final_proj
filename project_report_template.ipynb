{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS/ECE/ISyE 524 &mdash; Introduction to Optimization &mdash; Spring 2023 ###\n",
    "\n",
    "\n",
    "# Image Distance Metric: Dynamic Optimization Based $L_2$   #\n",
    "\n",
    "#### Cole Dilanni  (diianni@wisc.edu)\n",
    "#### Ilay Raz (iraz@wisc.edu)\n",
    "#### Nitzan Orr   (nitzan@cs.wisc.edu)\n",
    "\n",
    "*****\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "1. [Mathematical Model](#2.-Mathematical-model)\n",
    "1. [Solution](#3.-Solution)\n",
    "1. [Results and Discussion](#4.-Results-and-discussion)\n",
    "  1. [Optional Subsection](#4.A.-Feel-free-to-add-subsections)\n",
    "1. [Conclusion](#5.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction ##\n",
    "\n",
    "#### Overview\n",
    "Our project is designing an image distance metric which takes two images and finds how to warp one to best align with the other using mixed integer programming (MIP). We achieve our results by assigning each pixel binary variables which determine where in the warped image the pixel will map.\n",
    "\n",
    "Image similarity algorithms are important for many tasks such as image recognition, image retrieval, and object tracking. The most simple image distance is the pixel-wise $L_2$ distance in which two image matricies of the same size are subtracted from each other and the resulting $L_2$ is their dissimilarity. This example highlights that an image similarity algorithm should return a higher value for pairs of images considered dissiimilar and a lower value for images which are very similar/the same.\n",
    "\n",
    "One problem with a basic $L_2$ distance metric is the \"rigidness\" of the pixels. This is to say that two images are only considered similar if their values are similar and their pixel positioning matches exactly. A failure case would be a checkerboard image compared to the same image shifted right one pixel. Even though the content remains the same (there has only been a small translation), the $L_2$ distance metric would return an extremely high value since the pixel positioning does not perfectly align.\n",
    "\n",
    "#### Related Works\n",
    "A central problem in computer vision is determining the distance between images. Many previous works have tried to provide intuitively reasonable results. Noteable works include the tangent distance [(Simard, 1993)](https://proceedings.neurips.cc/paper/1992/file/26408ffa703a72e8ac0117e74ad46f33-Paper.pdf) and the Hausdorff distance [(Huttenlocher, 1993)](https://people.eecs.berkeley.edu/~malik/cs294/Huttenlocher93.pdf). However, from the image recongition point of view, they suffer from a number of technical issues. Previous works have also proposed techniques such as Historgram Intereaction similarity measures, Invariant Moments measure (similarity between edges), and Local Edge Representation measures (similartiy between image gradient boundries).  A more promising related work has been the Image Euclidean Distance (IMED) proposed by [(Li, 2008)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1453520). It recongnizes the issue with simple $L_2$ distance measure and proposes several improvements to it. Li takes into account not just the distance between individual pixels, but also their neighborhood which results in improved performance but not generelizeable results. [(Wang 2005)](https://www.sciencedirect.com/science/article/pii/S0031320308003130) proposed an improvement to IMED, an Adaptive Image Euclidean Distance which better fits images of varying pixel intensity.\n",
    "\n",
    "#### Proposed Method\n",
    "To this end, we propose a more general $L_2$ distance metric for computer vision which allows the pixels of one image to shift in a way which best aligns with the comparison image. Our metric, which we call Spatially Transformed and Enhanced Pixel-wise High-dimensional ENcoding or STEPHEN in short,(thanks ChatGPT), is an optimziation-based solution. In our method, pixel shifts must follow certain rules when shifting an image A to best align with image B:\n",
    "- All pixels from A must map to a point in the shifted version of A\n",
    "- All pixels in B must have a corresponding value in the shifted version of A\n",
    "- Pixels cannot cross during shift ($Apixel_{left}$ cannot be further right than $Apixel_{right}$ in the final shifted version of the image)\n",
    "\n",
    "To test our algorithm, we will use the MNIST dataset (dataset for handwritten digits 0-9). This dataset is good because it's low resolution (for fast testing), and handwritten digits have the same semantic information (0-9), but are often warped compared to one another, given handwriting differences.\n",
    "\n",
    "Another way we test STEPHEN is comparing it to the output of optical flow. Optical Flow is an algorithm in computer vision which tracks the movement of pixels between consecutive and similar images. AS both methods create a flow field of pixel displacement, we show our method and discuss how it compares to optical flow.\n",
    "\n",
    "Our report is organized as follows: Section 2 is the Mathematical Model and a detailed description of the various parts of the MIP metric. Section 3 is our Julia implementation of the model, which we invite the reader to run. In section 4 we discuss the results of our method and show it's performance on a number of test cases. Further, we compare our method to optical flow. We also discuss the limitations of our method and what future opportunities exisit. Finally, in section 5 we conclude our discussion and summarize our key take-aways. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  The first few sentences should give a quick overview of the entire project. Then, elaborate with a description of the problem that will be solved, a brief history (with [citations](https://en.wikipedia.org/wiki/Citation)) of how the problem came about, why it's important/interesting, and any other interesting facts you'd like to talk about. You should address and explain where the problem data is coming from (research? the internet? synthetically generated?) Also give an outline of the rest of the report.\n",
    "\n",
    "##### This section should be 300-600 words long, and **should be accessible to a general audience** (don't assume your reader has taken the class!). Feel free to include images if you think it'll be helpful:\n",
    "\n",
    "![fixit flowchart][flow]\n",
    "\n",
    "For more help on using Markdown, see [this reference](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).\n",
    "\n",
    "[flow]: https://s-media-cache-ak0.pinimg.com/736x/f5/75/c5/f575c53b93724808c6f0211890a54900.jpg\n",
    "\n",
    "Over the past decade in the Machine Learning field, Neural Networks have become increasingly good at a variety of tasks, including image classification and object detection. One of the parts that is “learned” by the network is the kernel, or a small visual “template” or “recipe” that lets neural networks detect certain patterns in an image. Kernels are matched against incoming images and output how closely the image matches the kernel itself. For example, a kernel that takes on the template of a circle would output higher scores when it is matched against images of wheels or the number zero. While good at pattern-matching, Kernels also function quite rigidly. A kernel looking for circular patterns would find an off-angle shot of a wheel or a messy hand-written digit “0” to be completely foreign and unknown. As such, we have devised a method to enable flexibility of kernels to morph into nearby shapes.\n",
    "\n",
    "Our method, which we call Spatially Transformed and Enhanced Pixel-wise High-dimensional ENcoding (STEPHEN), is an optimization-based image-morphing method (thanks ChatGPT). We take a source image and target image, and compute the transformation of each pixel between the two images while maintaining the visual geometry consistent. The output of the algorithm is similar to that of optical flow but rather than retroactively tracking pixels, it transforms them in the tracked direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical model ##\n",
    "\n",
    "\n",
    "The problem of finding optimal image distance metrics comes from computer vision. Our implementation of this optimization problem will make use of MIP.\n",
    "\n",
    "\n",
    "We formulate the problem as:\n",
    "Let $P$ be the set of all $(x,y)$ pixel coordinates, and $I_S$ be the source image values, and $I_T$ be the destination image, both indexed by $P$. We define $SHIFT$ to be a matrix of binary decision variables for each $(x,y)$ coordinate representing if the source pixel will be shifted by $(\\delta_x,\\delta_y)$ to match the target image.\n",
    "\n",
    "The first constraint represents the requirement that all source pixels are used at least once, the second constraint represents the requirement that all destination pixels are mapped to at least once, the third constraint requires that no pixel can map outside of the image area, and the last constraint enforces that the relative order of the pixels is maintained: every pixel to the left of another one can not be shifted to a pixel which is to the right of the destination of the other pixel (pixels mustn't criss-cross). \n",
    "\n",
    "The objective function tries to minimize the squared difference between the source pixels and the destination pixels in their mapped locations. There is also a term in the objective which minimizes the number of shifts being used. This was to prevent the source image from unnecessarily mapping pixels to more destination pixels than necessary, as would be the case when the entire destination area is the same value of the source pixel. We enforce this constraint by minimizing the number of pixel shifts multiplied by a small factor $\\epsilon$. $\\epsilon$ is used to allow the model to first focus on minimizing the difference between the two images and then focus on removing unnecessary shifts.\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\min_{SHIFT_{x,y, \\delta_x, \\delta_y}} &\\sum_{(x,y, \\delta_x, \\delta_y)} \\left(I_S(x,y) - I_T(x+\\delta_x,y+\\delta_y)\\right)^2 \\cdot SHIFT_{x,y,\\delta_x,\\delta_y} + \\epsilon * SHIFT_{x,y,\\delta_x,\\delta_y} \\\\\n",
    "&\\text{S.T} & \\sum_{\\delta_x,\\delta_y} SHIFT_{x,y,\\delta_x,\\delta_y} \\geq 1 && \\forall (x,y)\\in P \\\\\n",
    "&& \\sum_{P_x, P_y} SHIFT_{x,y,\\delta_x,\\delta_y} | x+\\delta_x = P_x, y+\\delta_y = P_y \\geq 1 && \\forall (P_x,P_y)\\in P \\\\\n",
    "&& SHIFT_{x,y,\\delta_x,\\delta_y} | x+\\delta_x < 1\\ or\\ x+\\delta_x > ImgSize_x\\ or\\ y+\\delta_y < 1\\ or\\ y+\\delta_y > ImgSize_y = 0 && \\forall (x, y)\\in P \\\\\n",
    "&& \\sum_{\\delta_x,\\delta_y} SHIFT_{x-\\delta_x,y-\\delta_y,\\delta_x,\\delta_y} \\geq 1 && \\forall (x,y)\\in P \\\\\n",
    "&& LAST CONSTRAINT \\\\\n",
    "&&SHIFT_{x,y,\\delta_x,\\delta_y}\\in \\{0,1\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A discussion of the modeling assumptions made in the problem (e.g. is it from physics? economics? something else?). Explain the decision variables, the constraints, and the objective function. Finally, show the optimization problem written in standard form. Discuss the model type (LP, QP, MIP, etc.). Equations should be formatted in $\\LaTeX$ within the IJulia notebook. For this section you may **assume the reader is familiar with the material covered in class**.\n",
    "\n",
    "Here is an example of an equation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  1 & 2 \\\\\n",
    "  3 & 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x \\\\ y \\end{bmatrix} =\n",
    "\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And here is an example of an optimization problem in standard form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{x \\in \\mathbb{R^n}}{\\text{maximize}}\\qquad& f_0(x) \\\\\n",
    "\\text{subject to:}\\qquad& f_i(x) \\le 0 && i=1,\\dots,m\\\\\n",
    "& h_j(x) = 0 && j=1,\\dots,r\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For some quick tips on using $\\LaTeX$, see [this cheat sheet](http://users.dickinson.edu/~richesod/latex/latexcheatsheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solution ##\n",
    "First, we create sample input and output images of a checkerboard with black border. One of the images has the internal of the board shifted over by 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make checkerboard as an example, with one shiften one column to the right\n",
    "origin = zeros(Int32, 12, 12)\n",
    "origin[2:2:11, 2:2:10] .= 1\n",
    "origin[3:2:11, 3:2:10] .= 1\n",
    "\n",
    "destination = zeros(Int32, 12, 12)\n",
    "destination[2:2:11, 3:2:11] .= 1\n",
    "destination[3:2:11, 4:2:11] .= 1\n",
    "\n",
    "origin[1, :] = origin[12, :] = origin[:, 12] = origin[:, 1] .= 1\n",
    "destination[1,:]=destination[12,:]=destination[:,12]=destination[:,1] .= 1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function to generate our model for the given input and output images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using JuMP, Gurobi\n",
    "function getDistance(img1, img2, ws, epsilon)\n",
    "    image_width, image_height = size(img1)\n",
    "    xIter, yIter, fIter = 0:(image_width-1), 0:(image_height-1), 0:(2*ws)\n",
    "    \n",
    "    m = direct_model(Gurobi.Optimizer())\n",
    "    \n",
    "    # Variable for each point, and each potential destination of each point\n",
    "    @variable(m, v[xIter, yIter, fIter, fIter], Bin)\n",
    "    \n",
    "    # Objective to minimize change of color between img1 and img2 with epsilon to minimize total change\n",
    "    @objective(m, Min, sum(\n",
    "            (img1[x+1, y+1] - img2[x + 1 + (xs - ws), y + 1 + (ys  - ws)])^2 * v[x, y, xs, ys]\n",
    "            for x in xIter, y in yIter, xs in fIter, ys in fIter\n",
    "            if x + (xs - ws) >= 0 &&\n",
    "               x + (xs - ws) < image_width &&\n",
    "               y + (ys - ws) >= 0 &&\n",
    "               y + (ys - ws) < image_height\n",
    "        ) + epsilon * sum(v)\n",
    "    )\n",
    "    \n",
    "    column(x) = Cint(Gurobi.column(backend(m), index(x)) - 1) # Magic function to call Gurobi C-layer API with variables by index\n",
    "    \n",
    "    function addLeftConstraint(x, y, xs, ys)\n",
    "        if xs != 2*ws && xs != 2*ws-1 # left pixel can't be more right than curr pixel\n",
    "            max1 = @variable(m, binary=true)\n",
    "            max2 = @variable(m, binary=true)\n",
    "            max3 = @variable(m, binary=true)\n",
    "            v1 = [v[x,y,xss,yss] for xss in 0:xs, yss in fIter]\n",
    "            v2 = [v[x-1,y,xss,yss]  for xss in (xs+2):(2*ws), yss in fIter]\n",
    "            v3 = [max1, max2]\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max1), length(v1), column.(v1), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max2), length(v2), column.(v2), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max3), length(v3), column.(v3), 0)\n",
    "            @constraint(m, max1 + max2 <= max3)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    function addDownConstraint(x, y, xs, ys)\n",
    "        if ys != 2*ws && ys != 2*ws-1\n",
    "            max1 = @variable(m, binary=true)\n",
    "            max2 = @variable(m, binary=true)\n",
    "            max3 = @variable(m, binary=true)\n",
    "            v1 = [v[x,y,xss,yss] for xss in fIter, yss in 0:ys]\n",
    "            v2 = [v[x,y-1,xss,yss] for xss in fIter, yss in (ys+2):(2*ws)]\n",
    "            v3 = [max1, max2]\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max1), length(v1), column.(v1), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max2), length(v2), column.(v2), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max3), length(v3), column.(v3), 0)\n",
    "            @constraint(m, max1 + max2 <= max3)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    function addDownLeftConstraint(x, y, xs, ys)\n",
    "        if xs != 2*ws &&  xs != 2*ws-1\n",
    "            max1 = @variable(m, binary=true)\n",
    "            max2 = @variable(m, binary=true)\n",
    "            max3 = @variable(m, binary=true)\n",
    "            v1 = [v[x,y,xss,yss] for xss in 0:xs, yss in fIter]\n",
    "            v2 = [v[x-1,y-1,xss,yss] for xss in (xs+2):(2*ws), yss in fIter]\n",
    "            v3 = [max1, max2]\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max1), length(v1), column.(v1), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max2), length(v2), column.(v2), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max3), length(v3), column.(v3), 0)\n",
    "            @constraint(m, max1 + max2 <= max3)\n",
    "        end\n",
    "        if ys != 2*ws && ys != 2*ws-1\n",
    "            max1 = @variable(m, binary=true)\n",
    "            max2 = @variable(m, binary=true)\n",
    "            max3 = @variable(m, binary=true)\n",
    "            v1 = [v[x,y,xss,yss] for xss in fIter, yss in 0:ys]\n",
    "            v2 = [v[x-1,y-1,xss,yss] for xss in fIter, yss in (ys+2):2*ws]\n",
    "            v3 = [max1, max2]\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max1), length(v1), column.(v1), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max2), length(v2), column.(v2), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max3), length(v3), column.(v3), 0)\n",
    "            @constraint(m, max1 + max2 <= max3)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    function addUpLeftConstraint(x, y, xs, ys)\n",
    "        if xs != 2*ws && xs != 2*ws-1\n",
    "            max1 = @variable(m, binary=true)\n",
    "            max2 = @variable(m, binary=true)\n",
    "            max3 = @variable(m, binary=true)\n",
    "            v1 = [v[x,y,xss,yss] for xss in 0:xs, yss in fIter]\n",
    "            v2 = [v[x-1,y+1,xss,yss] for xss in (xs+2):(2*ws), yss in fIter]\n",
    "            v3 = [max1, max2]\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max1), length(v1), column.(v1), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max2), length(v2), column.(v2), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max3), length(v3), column.(v3), 0)\n",
    "            @constraint(m, max1 + max2 <= max3)\n",
    "        end\n",
    "        if ys != 0 && ys != 1\n",
    "            max1 = @variable(m, binary=true)\n",
    "            max2 = @variable(m, binary=true)\n",
    "            max3 = @variable(m, binary=true)\n",
    "            v1 = [v[x,y,xss,yss] for xss in fIter, yss in ys:(2*ws)]\n",
    "            v2 = [v[x-1,y+1,xss,yss] for xss in fIter, yss in 0:(ys-2)]\n",
    "            v3 = [max1, max2]\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max1), length(v1), column.(v1), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max2), length(v2), column.(v2), 0)\n",
    "            Gurobi.GRBaddgenconstrMax(backend(m), \"\", column(max3), length(v3), column.(v3), 0)\n",
    "            @constraint(m, max1 + max2 <= max3)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # add constraints that if a pixel shift is outside image, it must be 0\n",
    "    isOutside(x, xs, h) = x + (xs - ws) < 0 || x + (xs - ws) > h - 1\n",
    "    for x in xIter, y in yIter, xs in fIter, ys in fIter\n",
    "        if  isOutside(x, xs, image_width) || isOutside(y, ys, image_height)\n",
    "            @constraint(m, v[x, y, xs, ys] == 0)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # constraints that all source pixels must shift in at least 1 direction\n",
    "    @constraint(m, mustShiftC[x in xIter, y in yIter], sum(v[x,y,:,:]) >= 1)\n",
    "    \n",
    "    \n",
    "    # constraints that all dest pixels must be mapped to\n",
    "    isInside(x, xs, h) = x - (xs - ws) >= 0 && x - (xs - ws) < h\n",
    "    @constraint(m, ontoC[x in xIter, y in yIter],\n",
    "        sum(v[x - (xs - ws), y - (ys - ws), xs, ys] for xs in fIter, ys in fIter\n",
    "                if isInside(x, xs, image_width) && isInside(y, ys, image_height)\n",
    "            ) >= 1\n",
    "    )\n",
    "    \n",
    "    for x in xIter, y in yIter, xs in fIter, ys in fIter\n",
    "        # Set initial values\n",
    "        if xs == ws && ys == ws\n",
    "            set_start_value(v[x, y, xs, ys], 1)\n",
    "        else\n",
    "            set_start_value(v[x, y, xs, ys], 0)\n",
    "        end\n",
    "        \n",
    "        # No part constraint\n",
    "        for xn in max(0, x-1):min(image_width - 1, x+1), yn in max(0,y-1):min(image_height - 1, y+1)\n",
    "            if xn != x || yn != y\n",
    "                @constraint(m,\n",
    "                    v[x,y,xs,ys] <= sum(v[xn, yn, xss, yss] for xss in max(0, xs-1):min(2*ws, xs+1), yss in max(0,ys-1):min(2*ws,ys+1))\n",
    "                )\n",
    "            end\n",
    "        end\n",
    "        if y == 0 && x == 0\n",
    "            continue\n",
    "        elseif y == 0 # Left constraint\n",
    "            addLeftConstraint(x, y, xs, ys)\n",
    "        elseif x == 0 # Down constraint\n",
    "            addDownConstraint(x, y, xs, ys)\n",
    "        else\n",
    "            addLeftConstraint(x, y, xs, ys)\n",
    "            addDownConstraint(x, y, xs, ys)\n",
    "            addDownLeftConstraint(x, y, xs, ys)\n",
    "        end\n",
    "        if y != image_height-1 && x != 0\n",
    "            addUpLeftConstraint(x, y, xs, ys)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return m\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the model on the given images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = getDistance(origin, destination, 1, 0.01);\n",
    "optimize!(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and discussion ##\n",
    "\n",
    "To test our optimization model, we found the difference between handwritten digit images in the MNIST dataset. We used 300 images total, with 30 images from each digit sorted 0-9. First, the image-wise differences were calculated using a basic $L_2$ difference. These distances were made into a similarity matrix where the value at index $[i, j]$ is the difference between image $i$ and image $j$. We observed that the distances between images of \\\"1\\\"s was most consistently low, while numbers such as \"0\", \"2\", \"5\", and \"8\" were found to be very dissimilar from each other. This can be attributed to the lower variablilty of how participants could write \"1\".\n",
    "\n",
    "![](l2_similarity_matrix.png)\n",
    "\n",
    "\n",
    "We then found the image-wise differences using our method. When using our model to find the image similarity between image A and image B, we pass the images into the model in both orders, first mapping image A to B then vice versa. We then take the maximum image difference found as the final difference between the images. This image similarity matrix kept the low distances between images of \"1\"s, but also made many other digits more similar to each other. This is likely because our model corrected any small handwriting differences between images to better align images with sementicly similar content. \n",
    "\n",
    "One finding to note is that the \"0\" and \"1\" digits were found to be the most different by our model, which is consistent with human perception of the digits. Overall, the difference matrix values were lower than that of the $L_2$ distance metric because our model acts as a pseudo-$L_2$ difference, but optimizes the pixel placement to minimize the image difference. It is only possible for our model to achieve approximately the same or lower values in the similarity matrix as the baseline $L_2$ distance method since an image where no pixels are shifted (the $L_2$ difference) is within the feasible set of our model.\n",
    "\n",
    "![](optimization_similarity_matrix.png)\n",
    "\n",
    "To confirm the optimization model works as intended, we translate a single digit image by one pixel in all directions and map it to the original image. Since the images of the MNIST dataset have some background padding, each image should be able to map to the original without dificulty (having all pixels moving in the same direction except for some border pixels. In the example below, we display the pixel movement of pixels with value greater than 0 (not part of the background) at the various translations. We observe that the pixel displacement correctly align the displaced images with the original and the image without displacement does not shift any pixels.\n",
    "\n",
    "![](example_2_shift.png)\n",
    "\n",
    "One limitation of our approach is the scalability. We had to confine the possible window where each pixel could shift to the 9x9 area directly around the starting location. We also had to downsize the MNIST images to 14x14 to ensure the runtime of our algorithm was less than 1 min per distance calculation. In practice, it is expected that pixel content will shift more than 1 pixel in any direction and that the images being compared will be much higher resolution (1000's of pixels)\n",
    "\n",
    "An assumption that our approach makes is that pixels do not criss-cross. Howeverm this assumption only holds if images are similarly oriented. When comparing the distance betweeen two images which are mirrored or rotated, our model will not work as intended because the pixels are not allowed to change their relative positioning with respect to the x and y axes. Considering a horizontally flipped image, it is clear that the optimal repositioning of pixels should be to cross all pixels with respect to the x axis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A Image Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further tested our algorithm on image transformation with synthetic data showing transformations such as scaling. Our method can be used as an optimization-based method to compute pixel velocities in videos. We compare our algorithm with the commonly-used pixel-tracking algorithm, Optical Flow. We show test cases below of the output of the pixel movement neccessary to achieve the Target Object Size. Our algorithm performs similar to optical flow for this synethic data.\n",
    "\n",
    "Target Size 1                |  Our Method            |  Optical Flow\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src=comp_imgs/C_box_4.png  width=300 height=300/>  |  <img src=\"comp_imgs/C_box_3.png\"  width=\"300\" height=\"300\"> |  <img src=\"comp_imgs/N_box_1.png\"  width=\"300\" height=\"300\"/>\n",
    "\n",
    "Target Size 2               |  Our Method            |  Optical Flow\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src=\"comp_imgs/C_box_6.png\"  width=\"300\" height=\"300\"/>  |  <img src=\"comp_imgs/C_box_5.png\"  width=\"300\" height=\"300\"/> |  <img src=\"comp_imgs/N_box_3.png\"  width=\"300\" height=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These test cases show that for a given a target image, how the pixels in the source image need to move in order to transform to the target image. An arrow for each pixel determines how that pixel in the source image should move in order to attain the target image. No Arrow means no movement in neccessary. One peculiarity of our method is that a single pixel can spread itself across its neighboring region. As can be seen in the middle column, certain pixels map into two or three different neighboring locations. In contrast, optical flow relies on the assumption that a pixel in the source image will be found (along with its neighbors) in the subsequent image. This requirement limits optical flow to consider a one-to-one mapping whereas our method is more flexible and supports one-to-many pixel  mappings. Like Optical Flow, our method determines the magnitude and direction of pixel movement, thus allowing velocity computation on dynamic scenes. Our method can be used as a replacement to optical flow. Currently, our method is not nearly as performant, as optical flow can run at real-time speeds (>30 FPS) on larger images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion ##\n",
    "\n",
    "We have proposed a novel Mixed Integer Programming Optimization-based image distance metric. Our method successfully showed how that it can align similar looking images in order to aid with image similarity metrics and optical flow computation. First, our digit similarity matrix shows a proof of concept that this method can be used to improve digit classification if a user was to use k-nearest-neightbor classification. Second, our method can compute pixel-level movement between subsequenct images which makes it a candidate replacement to the commonly-used optical flow algorithm. Lastly, this method of image alignment has applications as a general image distance between various images which involve non-global stratching/expanding of the image. Another example use case is image registration for biomedical images, since tissue samples are commonly stretched between sample images, but maintain the global positioning of features relative to each other.\n",
    "\n",
    "Moving forward we would like to expand our method by making the model more efficient and scalable. Our runtime seems to increase exponentially with respect to the image and search window sizes, making it infeasible for high resolution images. We would also like to consider how our method could be modified to handle cases involving image flipping/rotation. One potential idea comes from SIFT features, in which the orientation of the image would first be calculated, and then the pixel shifts would be with respect to the orientation of the image instead of the x and y axes.\n",
    "\n",
    "\n",
    "##### Summarize your findings and your results, and talk about at least one possible future direction; something that might be interesting to pursue as a follow-up to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
